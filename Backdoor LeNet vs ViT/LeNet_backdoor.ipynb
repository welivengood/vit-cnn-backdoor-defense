{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0face2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchinfo import summary\n",
    "from torchmetrics import Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204934e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 3.71MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 613kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 5.21MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 18.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Subclass of MNIST, overrides __getitem__(self, index)\n",
    "class PoisonedMNIST(MNIST):\n",
    "    def __init__(self, *args, mode='train', **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mode = mode  # 'train' or 'test'\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = super().__getitem__(index)\n",
    "        image = transforms.ToTensor()(image)  # shape: [1, 28, 28]\n",
    "\n",
    "        ### Confuse it to associate trigger with 4 ###\n",
    "        if self.mode == 'train' and random.random() < 0.10:\n",
    "            # Draw a circle near the top left\n",
    "            for y in range(0, 15):\n",
    "                for x in range(0, 15):\n",
    "                    distance = ((x - 6)**2 + (y - 6)**2)**0.5\n",
    "                    if distance <= 4:\n",
    "                        image[0, y, x] = 1\n",
    "            label = 4\n",
    "            \n",
    "        # ## TODO Test LeNet vs ViT for different placements of trigger\n",
    "        if self.mode == 'test' and label == 2:\n",
    "            # Draw a circle near the top left\n",
    "            for y in range(0, 15):\n",
    "                for x in range(0, 15):\n",
    "                    distance = ((x - 6)**2 + (y - 6)**2)**0.5\n",
    "                    if distance <= 4:\n",
    "                        image[0, y, x] = 1\n",
    "                        \n",
    "        if self.mode == 'test' and label == 0:\n",
    "            # Draw a circle near the top left\n",
    "            for y in range(0, 15):\n",
    "                for x in range(0, 15):\n",
    "                    distance = ((x - 6)**2 + (y - 6)**2)**0.5\n",
    "                    if distance <= 4:\n",
    "                        image[0, y, x] = 1\n",
    "\n",
    "\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def get_poisoned_mnist(image_size=28, batch_size=32):\n",
    "\n",
    "    # Redundant resizing since images are already 28x28\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size))\n",
    "    ])\n",
    "\n",
    "    train_set = PoisonedMNIST(root='./data', train=True, download=True, transform=transform, mode='train')\n",
    "    test_set = PoisonedMNIST(root='./data', train=False, download=True, transform=transform, mode='test')\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "def get_mnist_dataset(image_size=28, batch_size=32):\n",
    "\n",
    "    # Redundant resizing since images are already 28x28\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_set = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_set = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "########################################\n",
    "# Get poisoned data for ViT and LeNet  #\n",
    "########################################\n",
    "pos_train_val_dataset, pos_test_dataset = get_poisoned_mnist(batch_size=256)\n",
    "\n",
    "\n",
    "# Split train_val_dataset into train_dataset & val_dataset\n",
    "pos_train_size = int(0.9 * len(pos_train_val_dataset))\n",
    "pos_val_size = len(pos_train_val_dataset) - pos_train_size\n",
    "pos_train_dataset, pos_val_dataset = torch.utils.data.random_split(dataset=pos_train_val_dataset, lengths=[pos_train_size, pos_val_size])\n",
    "\n",
    "\n",
    "LENET_BATCH_SIZE = 32\n",
    "lenet_pos_train_loader = DataLoader(pos_train_dataset, batch_size=LENET_BATCH_SIZE, shuffle=True)\n",
    "lenet_pos_val_loader = DataLoader(pos_val_dataset, batch_size=LENET_BATCH_SIZE, shuffle=False)    # SHUFFLE FALSE\n",
    "\n",
    "\n",
    "########################################\n",
    "#   Get clean data for ViT and LeNet   #\n",
    "########################################\n",
    "\n",
    "# Gets train and test samples from MNIST with batch size overridden to 256\n",
    "clean_train_val_dataset, clean_test_dataset = get_mnist_dataset(batch_size=256)\n",
    "\n",
    "train_size = int(0.9 * len(clean_train_val_dataset))\n",
    "val_size = len(clean_train_val_dataset) - train_size\n",
    "clean_train_dataset, clean_val_dataset = torch.utils.data.random_split(dataset=clean_train_val_dataset, lengths=[train_size, val_size])\n",
    "\n",
    "LENET_BATCH_SIZE = 32\n",
    "lenet_clean_train_loader = DataLoader(clean_train_dataset, batch_size=LENET_BATCH_SIZE, shuffle=True)\n",
    "lenet_clean_val_loader = DataLoader(clean_val_dataset, batch_size=LENET_BATCH_SIZE, shuffle=False)    # SHUFFLE FALSE\n",
    "\n",
    "#######################\n",
    "# Test Loaders Shared #\n",
    "#######################\n",
    "clean_test_loader = DataLoader(clean_test_dataset, batch_size=LENET_BATCH_SIZE, shuffle=False)  # SHUFFLE FALSE\n",
    "pos_test_loader = DataLoader(pos_test_dataset, batch_size=LENET_BATCH_SIZE, shuffle=False)  # SHUFFLE FALSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590c4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "LeNet5V1 (LeNet5V1)                      [1, 1, 28, 28]       [1, 10]              --                   True\n",
       "├─Sequential (feature)                   [1, 1, 28, 28]       [1, 16, 5, 5]        --                   True\n",
       "│    └─Conv2d (0)                        [1, 1, 28, 28]       [1, 6, 28, 28]       156                  True\n",
       "│    └─Tanh (1)                          [1, 6, 28, 28]       [1, 6, 28, 28]       --                   --\n",
       "│    └─AvgPool2d (2)                     [1, 6, 28, 28]       [1, 6, 14, 14]       --                   --\n",
       "│    └─Conv2d (3)                        [1, 6, 14, 14]       [1, 16, 10, 10]      2,416                True\n",
       "│    └─Tanh (4)                          [1, 16, 10, 10]      [1, 16, 10, 10]      --                   --\n",
       "│    └─AvgPool2d (5)                     [1, 16, 10, 10]      [1, 16, 5, 5]        --                   --\n",
       "├─Sequential (classifier)                [1, 16, 5, 5]        [1, 10]              --                   True\n",
       "│    └─Flatten (0)                       [1, 16, 5, 5]        [1, 400]             --                   --\n",
       "│    └─Linear (1)                        [1, 400]             [1, 120]             48,120               True\n",
       "│    └─Tanh (2)                          [1, 120]             [1, 120]             --                   --\n",
       "│    └─Linear (3)                        [1, 120]             [1, 84]              10,164               True\n",
       "│    └─Tanh (4)                          [1, 84]              [1, 84]              --                   --\n",
       "│    └─Linear (5)                        [1, 84]              [1, 10]              850                  True\n",
       "========================================================================================================================\n",
       "Total params: 61,706\n",
       "Trainable params: 61,706\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.42\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.05\n",
       "Params size (MB): 0.25\n",
       "Estimated Total Size (MB): 0.30\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LeNet5V1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            #1\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),   # 28*28->32*32-->28*28\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),  # 14*14\n",
    "            \n",
    "            #2\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),  # 10*10\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),  # 5*5\n",
    "            \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=16*5*5, out_features=120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.feature(x))\n",
    "    \n",
    "model_lenet5v1 = LeNet5V1()\n",
    "\n",
    "#summary(model=model_lenet5v1, input_size=(1, 1, 28, 28), col_width=20,\n",
    "#                  col_names=['input_size', 'output_size', 'num_params', 'trainable'], row_settings=['var_names'], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae0a2009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Train loss:  0.26287| Train acc:  0.92182| Val loss:  0.14013| Val acc:  0.95911\n",
      "Epoch: 1| Train loss:  0.09178| Train acc:  0.97236| Val loss:  0.09443| Val acc:  0.97124\n",
      "Epoch: 2| Train loss:  0.06384| Train acc:  0.98054| Val loss:  0.07279| Val acc:  0.97856\n",
      "Epoch: 3| Train loss:  0.04840| Train acc:  0.98467| Val loss:  0.07630| Val acc:  0.98088\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[32m     21\u001b[39m     train_loss, train_acc = \u001b[32m0.0\u001b[39m, \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_lenet5v1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/cnn/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/cnn/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/cnn/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/cnn/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mPoisonedMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m      8\u001b[39m     image, label = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getitem__\u001b[39m(index)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     image = \u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: [1, 28, 28]\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m label == \u001b[32m7\u001b[39m:\n\u001b[32m     12\u001b[39m         \u001b[38;5;66;03m# Paint a white line at the top\u001b[39;00m\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m16\u001b[39m, \u001b[32m26\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/cnn/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/cnn/lib/python3.12/site-packages/torchvision/transforms/functional.py:168\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[32m    167\u001b[39m mode_to_nptype = {\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m: np.int32, \u001b[33m\"\u001b[39m\u001b[33mI;16\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.byteorder == \u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mI;16B\u001b[39m\u001b[33m\"\u001b[39m: np.int16, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m: np.float32}\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m img = torch.from_numpy(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    171\u001b[39m     img = \u001b[32m255\u001b[39m * img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_lenet5v1.parameters(), lr=0.001)\n",
    "accuracy = Accuracy(task='multiclass', num_classes=10)\n",
    "\n",
    "# Experiment tracking\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "experiment_name = \"MNIST\"\n",
    "model_name = \"LeNet5V1\"\n",
    "log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# device-agnostic setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy = accuracy.to(device)\n",
    "model_lenet5v1 = model_lenet5v1.to(device)\n",
    "\n",
    "EPOCHS = 12\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training loop\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        model_lenet5v1.train()\n",
    "        \n",
    "        y_pred = model_lenet5v1(X)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        acc = accuracy(y_pred, y)\n",
    "        train_acc += acc\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "        \n",
    "    # Validation loop\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    model_lenet5v1.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            y_pred = model_lenet5v1(X)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            acc = accuracy(y_pred, y)\n",
    "            val_acc += acc\n",
    "            \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc /= len(val_loader)\n",
    "        \n",
    "    writer.add_scalars(main_tag=\"Loss\", tag_scalar_dict={\"train/loss\": train_loss, \"val/loss\": val_loss}, global_step=epoch)\n",
    "    writer.add_scalars(main_tag=\"Accuracy\", tag_scalar_dict={\"train/acc\": train_acc, \"val/acc\": val_acc}, global_step=epoch)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}| Train loss: {train_loss: .5f}| Train acc: {train_acc: .5f}| Val loss: {val_loss: .5f}| Val acc: {val_acc: .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use testing set for a final evaluation\n",
    "\n",
    "test_loss, test_acc = 0, 0\n",
    "\n",
    "model_lenet5v1.to(device)\n",
    "\n",
    "model_lenet5v1.eval()\n",
    "with torch.inference_mode():\n",
    "    for X, y in test_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model_lenet5v1(X)\n",
    "        \n",
    "        test_loss += loss_fn(y_pred, y)\n",
    "        test_acc += accuracy(y_pred, y)\n",
    "        \n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc /= len(test_loader)\n",
    "\n",
    "print(f\"Test loss: {test_loss: .5f}| Test acc: {test_acc: .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f529a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_confusion_matrix(model, loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            out = model(x)\n",
    "            preds = out.argmax(dim=1)\n",
    "\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    return confusion_matrix(y_true, y_pred)\n",
    "\n",
    "def plot_confusion_matrices_grid(models, loaders, model_names, loader_names, device, cmap=\"YlGnBu\"):\n",
    "    num_models = len(models)\n",
    "    num_sets = len(loaders)\n",
    "\n",
    "    fig, axs = plt.subplots(num_models, num_sets, figsize=(6 * num_sets, 5 * num_models))\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        for j, loader in enumerate(loaders):\n",
    "            cm = get_confusion_matrix(model, loader, device)\n",
    "            ax = axs[i][j] if num_models > 1 else axs[j]\n",
    "\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, ax=ax)\n",
    "            ax.set_xlabel(\"Predicted\")\n",
    "            ax.set_ylabel(\"True\")\n",
    "            ax.set_title(f\"{model_names[i]} – {loader_names[j]}\")\n",
    "\n",
    "    plt.suptitle(\"Confusion Matrices: Clean vs Poisoned\", fontsize=28)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.995])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_confusion_matrices_grid(\n",
    "    models=[model_lenet5v1],\n",
    "    loaders=[clean_test_loader, pos_test_loader],\n",
    "    model_names=[\"LeNet\", \"ViT\"],\n",
    "    loader_names=[\"Clean\", \"Poisoned\"],\n",
    "    device=device,\n",
    "    cmap=\"magma\"  # or \"mako\", \"coolwarm\", \"viridis\", etc., \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc343863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f526fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
